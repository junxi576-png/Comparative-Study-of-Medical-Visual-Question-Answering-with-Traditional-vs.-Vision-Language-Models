import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig
from peft import PeftModel
from PIL import Image
import matplotlib.pyplot as plt
import os

# é…ç½®è·¯å¾„ | Path Configuration
MODEL_ID = "llava-hf/llava-1.5-7b-hf"
LORA_PATH = "./vqa_final_model"
# è¯·ç¡®ä¿è·¯å¾„æŒ‡å‘å®é™…å­˜åœ¨çš„åŒ»å­¦å›¾åƒ | Ensure the path points to an actual medical image
TEST_IMAGE = "data/VQA_RAD Image Folder/synpic19118.jpg" 

# 1. ä½¿ç”¨ 4-bit é‡åŒ–åŠ è½½åŸºç¡€æ¨¡å‹ | Load base model with 4-bit quantization
# è¿™æœ‰åŠ©äºèŠ‚çœæ˜¾å­˜å¹¶é¿å…æŸäº› ValueError | Helps save VRAM and avoid potential ValueErrors
print("æ­£åœ¨ä»¥ 4-bit æ¨¡å¼åŠ è½½æ¨¡å‹... | Loading model in 4-bit mode...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

base_model = LlavaForConditionalGeneration.from_pretrained(
    MODEL_ID, 
    quantization_config=bnb_config, 
    device_map="auto"
)

# 2. åŠ è½½ LoRA é€‚é…å±‚ä¸å¤„ç†å™¨ | Load LoRA adapter and processor
print("æ­£åœ¨åŠ è½½ LoRA æƒé‡... | Loading LoRA weights...")
model = PeftModel.from_pretrained(base_model, LORA_PATH)
processor = AutoProcessor.from_pretrained(MODEL_ID)

# 3. å‡†å¤‡æ¨ç†æ•°æ® | Prepare data for inference
if not os.path.exists(TEST_IMAGE):
    print(f"âŒ æ‰¾ä¸åˆ°å›¾ç‰‡: {TEST_IMAGE} | Image not found at the specified path.")
else:
    image = Image.open(TEST_IMAGE).convert("RGB")
    # LLaVA éœ€è¦ç‰¹å®šçš„ Prompt æ¨¡æ¿ï¼šUSER: <image>\nQuestion\nASSISTANT:
    # LLaVA requires a specific prompt template
    prompt = "USER: <image>\nWhat abnormality is present in this image?\nASSISTANT:"
    
    inputs = processor(text=prompt, images=image, return_tensors="pt").to("cuda")

    # 4. æ¨¡å‹æ¨ç†ç”Ÿæˆå›ç­” | Generate answer via model inference
    print("ğŸš€ æ¨¡å‹æ­£åœ¨ç”Ÿæˆå›ç­”... | Generating response...")
    with torch.inference_mode():
        output = model.generate(
            **inputs, 
            max_new_tokens=100, 
            do_sample=False # ä½¿ç”¨è´ªå©ªæœç´¢ä»¥ç¡®ä¿åŒ»ç–—é—®ç­”çš„ç¨³å®šæ€§ | Use Greedy Search for deterministic medical answers
        )
    
    # 5. è§£ç å¹¶æå–ç­”æ¡ˆ | Decode and extract the answer
    full_response = processor.decode(output[0], skip_special_tokens=True)
    # æå– ASSISTANT ä¹‹åçš„å†…å®¹ | Extract content following "ASSISTANT:"
    answer = full_response.split("ASSISTANT:")[-1].strip()

    print(f"\n--- æ¨ç†ç»“æœ Inference Result ---\né—®é¢˜ Question: What abnormality is present?\næ¨¡å‹å›ç­” Answer: {answer}\n")

    # 6. ç»“æœå¯è§†åŒ–ä¸ä¿å­˜ | Visualization and saving results
    plt.figure(figsize=(10, 7))
    plt.imshow(image)
    plt.title(f"VQA Case Study\nPredict: {answer}", fontsize=12, pad=15)
    plt.axis('off')
    plt.savefig('inference_result.png', bbox_inches='tight')
    plt.show()
    print("âœ… ç»“æœå›¾å·²ä¿å­˜ä¸º inference_result.png | Result image saved.")