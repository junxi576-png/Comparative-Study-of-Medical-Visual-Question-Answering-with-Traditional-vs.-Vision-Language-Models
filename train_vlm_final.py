import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration, Trainer, TrainingArguments, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model
from datasets import Dataset
from PIL import Image
import pandas as pd
import os

# 1. é…ç½® | Configuration
IMG_DIR = "data/VQA_RAD Image Folder"
TRAIN_CSV = "data/train.csv"
MODEL_ID = "llava-hf/llava-1.5-7b-hf"

# 2. åŠ è½½æ¨¡å‹ï¼šä½¿ç”¨ 4-bit é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ | Load Model: Using 4-bit quantization to save VRAM
print("æ­£åœ¨åŠ è½½æ¨¡å‹... | Loading model...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, 
    bnb_4bit_quant_type="nf4", 
    bnb_4bit_compute_dtype=torch.float16
)
model = LlavaForConditionalGeneration.from_pretrained(
    MODEL_ID, 
    quantization_config=bnb_config, 
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(MODEL_ID)

# 3. LoRA é…ç½®ï¼šåªå¾®è°ƒç‰¹å®šçš„æŠ•å½±å±‚ | LoRA Configuration: Fine-tune specific projection layers
# r=16: ç§©å¤§å°ï¼Œå¹³è¡¡è®­ç»ƒå‚æ•°é‡ä¸æ•ˆæœ | Rank size, balances params vs performance
lora_config = LoraConfig(
    r=16, 
    lora_alpha=32, 
    target_modules=["q_proj", "v_proj"], 
    lora_dropout=0.05, 
    bias="none", 
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# 4. æ•°æ®å‡†å¤‡ | Data Preparation
# ç¡®ä¿æ‰€æœ‰å†…å®¹ä¸ºå­—ç¬¦ä¸²å¹¶å¤„ç†ç¼ºå¤±å€¼ | Ensure strings and handle missing values
df = pd.read_csv(TRAIN_CSV).astype(str).replace('nan', 'missing')
train_dataset = Dataset.from_list(df.to_dict(orient="records"))

# 5. æ ¸å¿ƒä¿®å¤ï¼šä¿®æ”¹æ•°æ®æ•´ç†å‡½æ•° | Core Fix: Update Data Collator
def collate_fn(batch):
    """
    å°†å›¾åƒå’Œæ–‡æœ¬å¤„ç†ä¸ºæ¨¡å‹å¯æ¥å—çš„ Tensor
    Process images and text into tensors acceptable by the model
    """
    # éµå¾ª LLaVA çš„ Prompt æ¨¡æ¿ | Follow LLaVA Prompt Template
    texts = [f"USER: <image>\n{item['question_clean']}\nASSISTANT: {item['answer_clean']}" for item in batch]
    images = [Image.open(os.path.join(IMG_DIR, item['image_name'])).convert("RGB") for item in batch]
    
    # å…³é”®ç‚¹ï¼šLLaVA çš„å›¾åƒä¼šè¢«ç¼–ç ä¸º 576 ä¸ª Token
    # Key point: LLaVA images are encoded into 576 tokens
    inputs = processor(
        text=texts, 
        images=images, 
        return_tensors="pt", 
        padding=True, 
        truncation=True, 
        max_length=800  # 576 (image) + extra space for text
    )
    
    # labels ç”¨äºè®¡ç®—äº¤å‰ç†µæŸå¤± | Labels used for calculating Cross-Entropy loss
    inputs["labels"] = inputs["input_ids"].clone()
    return inputs

# 6. è®­ç»ƒå‚æ•°è®¾ç½® | Training Arguments
training_args = TrainingArguments(
    output_dir="./llava-vqa-results",
    per_device_train_batch_size=1,      # å‡å° Batch Size é˜²æ­¢æ˜¾å­˜æº¢å‡º | Reduce BS to prevent OOM
    gradient_accumulation_steps=8,      # æ¢¯åº¦ç´¯ç§¯ç»´æŒç­‰æ•ˆ BS=8 | Gradient accumulation for effective BS=8
    learning_rate=2e-4,
    num_train_epochs=3,
    fp16=True,                          # å¼€å¯åŠç²¾åº¦åŠ é€Ÿ | Enable mixed precision
    logging_steps=5,
    save_strategy="epoch",
    remove_unused_columns=False,        # å¿…é¡»è®¾ä¸º False ä»¥ä¿ç•™å›¾åƒæ•°æ® | Must be False to keep image data
    report_to="none"
)

# 7. åˆå§‹åŒ–è®­ç»ƒå™¨ | Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=collate_fn
)

print("ğŸš€ é‡æ–°å¯åŠ¨å¾®è°ƒ (å·²ä¿®å¤ Token åŒ¹é…é—®é¢˜)... | Restarting fine-tuning...")


try:
    trainer.train()
    # ä¿å­˜å¾®è°ƒåçš„ LoRA é€‚é…å™¨ | Save the fine-tuned LoRA adapter
    model.save_pretrained("./vqa_final_model")
    print("âœ… è®­ç»ƒå®Œæˆï¼ | Training Complete!")
except Exception as e:
    print(f"âŒ é”™è¯¯ Error: {e}")