{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 394,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02542911633820725,
      "grad_norm": 10.014010429382324,
      "learning_rate": 0.00019864636209813877,
      "loss": 12.74,
      "step": 5
    },
    {
      "epoch": 0.0508582326764145,
      "grad_norm": 8.64881706237793,
      "learning_rate": 0.00019695431472081218,
      "loss": 8.0155,
      "step": 10
    },
    {
      "epoch": 0.07628734901462174,
      "grad_norm": 1.4331430196762085,
      "learning_rate": 0.00019526226734348562,
      "loss": 4.924,
      "step": 15
    },
    {
      "epoch": 0.101716465352829,
      "grad_norm": 0.6661530137062073,
      "learning_rate": 0.00019357021996615906,
      "loss": 4.4623,
      "step": 20
    },
    {
      "epoch": 0.12714558169103624,
      "grad_norm": 0.5092810392379761,
      "learning_rate": 0.0001918781725888325,
      "loss": 4.2832,
      "step": 25
    },
    {
      "epoch": 0.15257469802924348,
      "grad_norm": 0.2215346395969391,
      "learning_rate": 0.00019018612521150594,
      "loss": 4.1912,
      "step": 30
    },
    {
      "epoch": 0.17800381436745072,
      "grad_norm": 0.29600244760513306,
      "learning_rate": 0.00018849407783417936,
      "loss": 4.1328,
      "step": 35
    },
    {
      "epoch": 0.203432930705658,
      "grad_norm": 0.27767401933670044,
      "learning_rate": 0.0001868020304568528,
      "loss": 4.0631,
      "step": 40
    },
    {
      "epoch": 0.22886204704386523,
      "grad_norm": 0.2678418755531311,
      "learning_rate": 0.00018510998307952624,
      "loss": 3.9982,
      "step": 45
    },
    {
      "epoch": 0.25429116338207247,
      "grad_norm": 0.22603389620780945,
      "learning_rate": 0.00018341793570219968,
      "loss": 3.939,
      "step": 50
    },
    {
      "epoch": 0.27972027972027974,
      "grad_norm": 0.17454132437705994,
      "learning_rate": 0.0001817258883248731,
      "loss": 3.8878,
      "step": 55
    },
    {
      "epoch": 0.30514939605848695,
      "grad_norm": 0.23212780058383942,
      "learning_rate": 0.00018003384094754653,
      "loss": 3.8501,
      "step": 60
    },
    {
      "epoch": 0.3305785123966942,
      "grad_norm": 0.26888999342918396,
      "learning_rate": 0.00017834179357021997,
      "loss": 3.8254,
      "step": 65
    },
    {
      "epoch": 0.35600762873490144,
      "grad_norm": 0.3695148229598999,
      "learning_rate": 0.0001766497461928934,
      "loss": 3.8125,
      "step": 70
    },
    {
      "epoch": 0.3814367450731087,
      "grad_norm": 0.1405540406703949,
      "learning_rate": 0.00017495769881556685,
      "loss": 3.7988,
      "step": 75
    },
    {
      "epoch": 0.406865861411316,
      "grad_norm": 0.16706888377666473,
      "learning_rate": 0.00017326565143824027,
      "loss": 3.7829,
      "step": 80
    },
    {
      "epoch": 0.4322949777495232,
      "grad_norm": 0.1048629954457283,
      "learning_rate": 0.0001715736040609137,
      "loss": 3.7787,
      "step": 85
    },
    {
      "epoch": 0.45772409408773046,
      "grad_norm": 0.17084570229053497,
      "learning_rate": 0.00016988155668358715,
      "loss": 3.7735,
      "step": 90
    },
    {
      "epoch": 0.4831532104259377,
      "grad_norm": 0.189347043633461,
      "learning_rate": 0.0001681895093062606,
      "loss": 3.7652,
      "step": 95
    },
    {
      "epoch": 0.5085823267641449,
      "grad_norm": 0.15642620623111725,
      "learning_rate": 0.00016649746192893403,
      "loss": 3.7596,
      "step": 100
    },
    {
      "epoch": 0.5340114431023522,
      "grad_norm": 0.26023373007774353,
      "learning_rate": 0.00016480541455160744,
      "loss": 3.7673,
      "step": 105
    },
    {
      "epoch": 0.5594405594405595,
      "grad_norm": 0.3786662817001343,
      "learning_rate": 0.00016311336717428088,
      "loss": 3.7626,
      "step": 110
    },
    {
      "epoch": 0.5848696757787667,
      "grad_norm": 0.11224820464849472,
      "learning_rate": 0.00016142131979695432,
      "loss": 3.759,
      "step": 115
    },
    {
      "epoch": 0.6102987921169739,
      "grad_norm": 0.1926216334104538,
      "learning_rate": 0.00015972927241962776,
      "loss": 3.7557,
      "step": 120
    },
    {
      "epoch": 0.6357279084551812,
      "grad_norm": 0.08838542550802231,
      "learning_rate": 0.00015803722504230118,
      "loss": 3.7552,
      "step": 125
    },
    {
      "epoch": 0.6611570247933884,
      "grad_norm": 0.11044508963823318,
      "learning_rate": 0.00015634517766497462,
      "loss": 3.7526,
      "step": 130
    },
    {
      "epoch": 0.6865861411315957,
      "grad_norm": 0.1150602474808693,
      "learning_rate": 0.00015465313028764806,
      "loss": 3.7538,
      "step": 135
    },
    {
      "epoch": 0.7120152574698029,
      "grad_norm": 0.08761288970708847,
      "learning_rate": 0.0001529610829103215,
      "loss": 3.7564,
      "step": 140
    },
    {
      "epoch": 0.7374443738080102,
      "grad_norm": 0.10867762565612793,
      "learning_rate": 0.00015126903553299494,
      "loss": 3.7509,
      "step": 145
    },
    {
      "epoch": 0.7628734901462174,
      "grad_norm": 0.18101619184017181,
      "learning_rate": 0.00014957698815566835,
      "loss": 3.7558,
      "step": 150
    },
    {
      "epoch": 0.7883026064844246,
      "grad_norm": 0.13764391839504242,
      "learning_rate": 0.0001478849407783418,
      "loss": 3.7511,
      "step": 155
    },
    {
      "epoch": 0.813731722822632,
      "grad_norm": 0.2041599005460739,
      "learning_rate": 0.00014619289340101523,
      "loss": 3.7515,
      "step": 160
    },
    {
      "epoch": 0.8391608391608392,
      "grad_norm": 0.10715719312429428,
      "learning_rate": 0.00014450084602368868,
      "loss": 3.747,
      "step": 165
    },
    {
      "epoch": 0.8645899554990464,
      "grad_norm": 0.09191086888313293,
      "learning_rate": 0.0001428087986463621,
      "loss": 3.7525,
      "step": 170
    },
    {
      "epoch": 0.8900190718372537,
      "grad_norm": 0.12299282848834991,
      "learning_rate": 0.00014111675126903553,
      "loss": 3.7463,
      "step": 175
    },
    {
      "epoch": 0.9154481881754609,
      "grad_norm": 0.10152900218963623,
      "learning_rate": 0.000139424703891709,
      "loss": 3.7544,
      "step": 180
    },
    {
      "epoch": 0.9408773045136681,
      "grad_norm": 0.10348465293645859,
      "learning_rate": 0.0001377326565143824,
      "loss": 3.7493,
      "step": 185
    },
    {
      "epoch": 0.9663064208518753,
      "grad_norm": 0.09588255733251572,
      "learning_rate": 0.00013604060913705585,
      "loss": 3.7496,
      "step": 190
    },
    {
      "epoch": 0.9917355371900827,
      "grad_norm": 0.12192687392234802,
      "learning_rate": 0.00013434856175972926,
      "loss": 3.7489,
      "step": 195
    },
    {
      "epoch": 1.0152574698029244,
      "grad_norm": 0.09682174026966095,
      "learning_rate": 0.00013265651438240273,
      "loss": 3.747,
      "step": 200
    },
    {
      "epoch": 1.0406865861411316,
      "grad_norm": 0.08611093461513519,
      "learning_rate": 0.00013096446700507615,
      "loss": 3.7439,
      "step": 205
    },
    {
      "epoch": 1.0661157024793388,
      "grad_norm": 0.08872659504413605,
      "learning_rate": 0.00012927241962774959,
      "loss": 3.7444,
      "step": 210
    },
    {
      "epoch": 1.091544818817546,
      "grad_norm": 0.07425019890069962,
      "learning_rate": 0.000127580372250423,
      "loss": 3.7458,
      "step": 215
    },
    {
      "epoch": 1.1169739351557533,
      "grad_norm": 0.08875197917222977,
      "learning_rate": 0.00012588832487309644,
      "loss": 3.7525,
      "step": 220
    },
    {
      "epoch": 1.1424030514939605,
      "grad_norm": 0.07381638884544373,
      "learning_rate": 0.0001241962774957699,
      "loss": 3.7456,
      "step": 225
    },
    {
      "epoch": 1.167832167832168,
      "grad_norm": 0.0744028091430664,
      "learning_rate": 0.00012250423011844332,
      "loss": 3.7476,
      "step": 230
    },
    {
      "epoch": 1.1932612841703751,
      "grad_norm": 0.09087807685136795,
      "learning_rate": 0.00012081218274111676,
      "loss": 3.7503,
      "step": 235
    },
    {
      "epoch": 1.2186904005085823,
      "grad_norm": 0.08302421122789383,
      "learning_rate": 0.00011912013536379019,
      "loss": 3.7427,
      "step": 240
    },
    {
      "epoch": 1.2441195168467896,
      "grad_norm": 0.12529940903186798,
      "learning_rate": 0.00011742808798646363,
      "loss": 3.747,
      "step": 245
    },
    {
      "epoch": 1.2695486331849968,
      "grad_norm": 0.09080091118812561,
      "learning_rate": 0.00011573604060913706,
      "loss": 3.745,
      "step": 250
    },
    {
      "epoch": 1.294977749523204,
      "grad_norm": 0.0780387669801712,
      "learning_rate": 0.0001140439932318105,
      "loss": 3.7395,
      "step": 255
    },
    {
      "epoch": 1.3204068658614112,
      "grad_norm": 0.08573130518198013,
      "learning_rate": 0.00011235194585448392,
      "loss": 3.7472,
      "step": 260
    },
    {
      "epoch": 1.3458359821996186,
      "grad_norm": 0.08145179599523544,
      "learning_rate": 0.00011065989847715736,
      "loss": 3.7472,
      "step": 265
    },
    {
      "epoch": 1.3712650985378259,
      "grad_norm": 0.0782041922211647,
      "learning_rate": 0.0001089678510998308,
      "loss": 3.7482,
      "step": 270
    },
    {
      "epoch": 1.396694214876033,
      "grad_norm": 0.0893397107720375,
      "learning_rate": 0.00010727580372250423,
      "loss": 3.7478,
      "step": 275
    },
    {
      "epoch": 1.4221233312142403,
      "grad_norm": 0.08886317163705826,
      "learning_rate": 0.00010558375634517767,
      "loss": 3.7491,
      "step": 280
    },
    {
      "epoch": 1.4475524475524475,
      "grad_norm": 0.07857801765203476,
      "learning_rate": 0.0001038917089678511,
      "loss": 3.7486,
      "step": 285
    },
    {
      "epoch": 1.472981563890655,
      "grad_norm": 0.07587987184524536,
      "learning_rate": 0.00010219966159052454,
      "loss": 3.7427,
      "step": 290
    },
    {
      "epoch": 1.498410680228862,
      "grad_norm": 0.07942982763051987,
      "learning_rate": 0.00010050761421319797,
      "loss": 3.7493,
      "step": 295
    },
    {
      "epoch": 1.5238397965670694,
      "grad_norm": 0.09064285457134247,
      "learning_rate": 9.881556683587141e-05,
      "loss": 3.7452,
      "step": 300
    },
    {
      "epoch": 1.5492689129052766,
      "grad_norm": 0.08269606530666351,
      "learning_rate": 9.712351945854485e-05,
      "loss": 3.7496,
      "step": 305
    },
    {
      "epoch": 1.5746980292434838,
      "grad_norm": 0.090977244079113,
      "learning_rate": 9.543147208121828e-05,
      "loss": 3.7417,
      "step": 310
    },
    {
      "epoch": 1.600127145581691,
      "grad_norm": 0.06558023393154144,
      "learning_rate": 9.373942470389172e-05,
      "loss": 3.7459,
      "step": 315
    },
    {
      "epoch": 1.6255562619198982,
      "grad_norm": 0.08327018469572067,
      "learning_rate": 9.204737732656514e-05,
      "loss": 3.7466,
      "step": 320
    },
    {
      "epoch": 1.6509853782581057,
      "grad_norm": 0.08627612888813019,
      "learning_rate": 9.035532994923858e-05,
      "loss": 3.7425,
      "step": 325
    },
    {
      "epoch": 1.6764144945963126,
      "grad_norm": 0.09870980679988861,
      "learning_rate": 8.866328257191202e-05,
      "loss": 3.7447,
      "step": 330
    },
    {
      "epoch": 1.70184361093452,
      "grad_norm": 0.08487703651189804,
      "learning_rate": 8.697123519458545e-05,
      "loss": 3.7436,
      "step": 335
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.07920721918344498,
      "learning_rate": 8.527918781725889e-05,
      "loss": 3.7456,
      "step": 340
    },
    {
      "epoch": 1.7527018436109345,
      "grad_norm": 0.10429544001817703,
      "learning_rate": 8.358714043993232e-05,
      "loss": 3.7474,
      "step": 345
    },
    {
      "epoch": 1.7781309599491417,
      "grad_norm": 0.10490645468235016,
      "learning_rate": 8.189509306260576e-05,
      "loss": 3.7484,
      "step": 350
    },
    {
      "epoch": 1.803560076287349,
      "grad_norm": 0.1072736531496048,
      "learning_rate": 8.020304568527919e-05,
      "loss": 3.7454,
      "step": 355
    },
    {
      "epoch": 1.8289891926255564,
      "grad_norm": 0.07580262422561646,
      "learning_rate": 7.851099830795263e-05,
      "loss": 3.7407,
      "step": 360
    },
    {
      "epoch": 1.8544183089637634,
      "grad_norm": 0.0910833403468132,
      "learning_rate": 7.681895093062607e-05,
      "loss": 3.7473,
      "step": 365
    },
    {
      "epoch": 1.8798474253019708,
      "grad_norm": 0.09519973397254944,
      "learning_rate": 7.51269035532995e-05,
      "loss": 3.7431,
      "step": 370
    },
    {
      "epoch": 1.905276541640178,
      "grad_norm": 0.11287260800600052,
      "learning_rate": 7.343485617597293e-05,
      "loss": 3.7433,
      "step": 375
    },
    {
      "epoch": 1.9307056579783852,
      "grad_norm": 0.08734976500272751,
      "learning_rate": 7.174280879864636e-05,
      "loss": 3.7483,
      "step": 380
    },
    {
      "epoch": 1.9561347743165927,
      "grad_norm": 0.11192352324724197,
      "learning_rate": 7.00507614213198e-05,
      "loss": 3.7437,
      "step": 385
    },
    {
      "epoch": 1.9815638906547997,
      "grad_norm": 0.09055870771408081,
      "learning_rate": 6.835871404399323e-05,
      "loss": 3.7459,
      "step": 390
    }
  ],
  "logging_steps": 5,
  "max_steps": 591,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.85641786278912e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
