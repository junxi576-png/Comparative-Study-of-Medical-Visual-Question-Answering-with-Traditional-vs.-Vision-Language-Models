{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 591,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02542911633820725,
      "grad_norm": 10.014010429382324,
      "learning_rate": 0.00019864636209813877,
      "loss": 12.74,
      "step": 5
    },
    {
      "epoch": 0.0508582326764145,
      "grad_norm": 8.64881706237793,
      "learning_rate": 0.00019695431472081218,
      "loss": 8.0155,
      "step": 10
    },
    {
      "epoch": 0.07628734901462174,
      "grad_norm": 1.4331430196762085,
      "learning_rate": 0.00019526226734348562,
      "loss": 4.924,
      "step": 15
    },
    {
      "epoch": 0.101716465352829,
      "grad_norm": 0.6661530137062073,
      "learning_rate": 0.00019357021996615906,
      "loss": 4.4623,
      "step": 20
    },
    {
      "epoch": 0.12714558169103624,
      "grad_norm": 0.5092810392379761,
      "learning_rate": 0.0001918781725888325,
      "loss": 4.2832,
      "step": 25
    },
    {
      "epoch": 0.15257469802924348,
      "grad_norm": 0.2215346395969391,
      "learning_rate": 0.00019018612521150594,
      "loss": 4.1912,
      "step": 30
    },
    {
      "epoch": 0.17800381436745072,
      "grad_norm": 0.29600244760513306,
      "learning_rate": 0.00018849407783417936,
      "loss": 4.1328,
      "step": 35
    },
    {
      "epoch": 0.203432930705658,
      "grad_norm": 0.27767401933670044,
      "learning_rate": 0.0001868020304568528,
      "loss": 4.0631,
      "step": 40
    },
    {
      "epoch": 0.22886204704386523,
      "grad_norm": 0.2678418755531311,
      "learning_rate": 0.00018510998307952624,
      "loss": 3.9982,
      "step": 45
    },
    {
      "epoch": 0.25429116338207247,
      "grad_norm": 0.22603389620780945,
      "learning_rate": 0.00018341793570219968,
      "loss": 3.939,
      "step": 50
    },
    {
      "epoch": 0.27972027972027974,
      "grad_norm": 0.17454132437705994,
      "learning_rate": 0.0001817258883248731,
      "loss": 3.8878,
      "step": 55
    },
    {
      "epoch": 0.30514939605848695,
      "grad_norm": 0.23212780058383942,
      "learning_rate": 0.00018003384094754653,
      "loss": 3.8501,
      "step": 60
    },
    {
      "epoch": 0.3305785123966942,
      "grad_norm": 0.26888999342918396,
      "learning_rate": 0.00017834179357021997,
      "loss": 3.8254,
      "step": 65
    },
    {
      "epoch": 0.35600762873490144,
      "grad_norm": 0.3695148229598999,
      "learning_rate": 0.0001766497461928934,
      "loss": 3.8125,
      "step": 70
    },
    {
      "epoch": 0.3814367450731087,
      "grad_norm": 0.1405540406703949,
      "learning_rate": 0.00017495769881556685,
      "loss": 3.7988,
      "step": 75
    },
    {
      "epoch": 0.406865861411316,
      "grad_norm": 0.16706888377666473,
      "learning_rate": 0.00017326565143824027,
      "loss": 3.7829,
      "step": 80
    },
    {
      "epoch": 0.4322949777495232,
      "grad_norm": 0.1048629954457283,
      "learning_rate": 0.0001715736040609137,
      "loss": 3.7787,
      "step": 85
    },
    {
      "epoch": 0.45772409408773046,
      "grad_norm": 0.17084570229053497,
      "learning_rate": 0.00016988155668358715,
      "loss": 3.7735,
      "step": 90
    },
    {
      "epoch": 0.4831532104259377,
      "grad_norm": 0.189347043633461,
      "learning_rate": 0.0001681895093062606,
      "loss": 3.7652,
      "step": 95
    },
    {
      "epoch": 0.5085823267641449,
      "grad_norm": 0.15642620623111725,
      "learning_rate": 0.00016649746192893403,
      "loss": 3.7596,
      "step": 100
    },
    {
      "epoch": 0.5340114431023522,
      "grad_norm": 0.26023373007774353,
      "learning_rate": 0.00016480541455160744,
      "loss": 3.7673,
      "step": 105
    },
    {
      "epoch": 0.5594405594405595,
      "grad_norm": 0.3786662817001343,
      "learning_rate": 0.00016311336717428088,
      "loss": 3.7626,
      "step": 110
    },
    {
      "epoch": 0.5848696757787667,
      "grad_norm": 0.11224820464849472,
      "learning_rate": 0.00016142131979695432,
      "loss": 3.759,
      "step": 115
    },
    {
      "epoch": 0.6102987921169739,
      "grad_norm": 0.1926216334104538,
      "learning_rate": 0.00015972927241962776,
      "loss": 3.7557,
      "step": 120
    },
    {
      "epoch": 0.6357279084551812,
      "grad_norm": 0.08838542550802231,
      "learning_rate": 0.00015803722504230118,
      "loss": 3.7552,
      "step": 125
    },
    {
      "epoch": 0.6611570247933884,
      "grad_norm": 0.11044508963823318,
      "learning_rate": 0.00015634517766497462,
      "loss": 3.7526,
      "step": 130
    },
    {
      "epoch": 0.6865861411315957,
      "grad_norm": 0.1150602474808693,
      "learning_rate": 0.00015465313028764806,
      "loss": 3.7538,
      "step": 135
    },
    {
      "epoch": 0.7120152574698029,
      "grad_norm": 0.08761288970708847,
      "learning_rate": 0.0001529610829103215,
      "loss": 3.7564,
      "step": 140
    },
    {
      "epoch": 0.7374443738080102,
      "grad_norm": 0.10867762565612793,
      "learning_rate": 0.00015126903553299494,
      "loss": 3.7509,
      "step": 145
    },
    {
      "epoch": 0.7628734901462174,
      "grad_norm": 0.18101619184017181,
      "learning_rate": 0.00014957698815566835,
      "loss": 3.7558,
      "step": 150
    },
    {
      "epoch": 0.7883026064844246,
      "grad_norm": 0.13764391839504242,
      "learning_rate": 0.0001478849407783418,
      "loss": 3.7511,
      "step": 155
    },
    {
      "epoch": 0.813731722822632,
      "grad_norm": 0.2041599005460739,
      "learning_rate": 0.00014619289340101523,
      "loss": 3.7515,
      "step": 160
    },
    {
      "epoch": 0.8391608391608392,
      "grad_norm": 0.10715719312429428,
      "learning_rate": 0.00014450084602368868,
      "loss": 3.747,
      "step": 165
    },
    {
      "epoch": 0.8645899554990464,
      "grad_norm": 0.09191086888313293,
      "learning_rate": 0.0001428087986463621,
      "loss": 3.7525,
      "step": 170
    },
    {
      "epoch": 0.8900190718372537,
      "grad_norm": 0.12299282848834991,
      "learning_rate": 0.00014111675126903553,
      "loss": 3.7463,
      "step": 175
    },
    {
      "epoch": 0.9154481881754609,
      "grad_norm": 0.10152900218963623,
      "learning_rate": 0.000139424703891709,
      "loss": 3.7544,
      "step": 180
    },
    {
      "epoch": 0.9408773045136681,
      "grad_norm": 0.10348465293645859,
      "learning_rate": 0.0001377326565143824,
      "loss": 3.7493,
      "step": 185
    },
    {
      "epoch": 0.9663064208518753,
      "grad_norm": 0.09588255733251572,
      "learning_rate": 0.00013604060913705585,
      "loss": 3.7496,
      "step": 190
    },
    {
      "epoch": 0.9917355371900827,
      "grad_norm": 0.12192687392234802,
      "learning_rate": 0.00013434856175972926,
      "loss": 3.7489,
      "step": 195
    },
    {
      "epoch": 1.0152574698029244,
      "grad_norm": 0.09682174026966095,
      "learning_rate": 0.00013265651438240273,
      "loss": 3.747,
      "step": 200
    },
    {
      "epoch": 1.0406865861411316,
      "grad_norm": 0.08611093461513519,
      "learning_rate": 0.00013096446700507615,
      "loss": 3.7439,
      "step": 205
    },
    {
      "epoch": 1.0661157024793388,
      "grad_norm": 0.08872659504413605,
      "learning_rate": 0.00012927241962774959,
      "loss": 3.7444,
      "step": 210
    },
    {
      "epoch": 1.091544818817546,
      "grad_norm": 0.07425019890069962,
      "learning_rate": 0.000127580372250423,
      "loss": 3.7458,
      "step": 215
    },
    {
      "epoch": 1.1169739351557533,
      "grad_norm": 0.08875197917222977,
      "learning_rate": 0.00012588832487309644,
      "loss": 3.7525,
      "step": 220
    },
    {
      "epoch": 1.1424030514939605,
      "grad_norm": 0.07381638884544373,
      "learning_rate": 0.0001241962774957699,
      "loss": 3.7456,
      "step": 225
    },
    {
      "epoch": 1.167832167832168,
      "grad_norm": 0.0744028091430664,
      "learning_rate": 0.00012250423011844332,
      "loss": 3.7476,
      "step": 230
    },
    {
      "epoch": 1.1932612841703751,
      "grad_norm": 0.09087807685136795,
      "learning_rate": 0.00012081218274111676,
      "loss": 3.7503,
      "step": 235
    },
    {
      "epoch": 1.2186904005085823,
      "grad_norm": 0.08302421122789383,
      "learning_rate": 0.00011912013536379019,
      "loss": 3.7427,
      "step": 240
    },
    {
      "epoch": 1.2441195168467896,
      "grad_norm": 0.12529940903186798,
      "learning_rate": 0.00011742808798646363,
      "loss": 3.747,
      "step": 245
    },
    {
      "epoch": 1.2695486331849968,
      "grad_norm": 0.09080091118812561,
      "learning_rate": 0.00011573604060913706,
      "loss": 3.745,
      "step": 250
    },
    {
      "epoch": 1.294977749523204,
      "grad_norm": 0.0780387669801712,
      "learning_rate": 0.0001140439932318105,
      "loss": 3.7395,
      "step": 255
    },
    {
      "epoch": 1.3204068658614112,
      "grad_norm": 0.08573130518198013,
      "learning_rate": 0.00011235194585448392,
      "loss": 3.7472,
      "step": 260
    },
    {
      "epoch": 1.3458359821996186,
      "grad_norm": 0.08145179599523544,
      "learning_rate": 0.00011065989847715736,
      "loss": 3.7472,
      "step": 265
    },
    {
      "epoch": 1.3712650985378259,
      "grad_norm": 0.0782041922211647,
      "learning_rate": 0.0001089678510998308,
      "loss": 3.7482,
      "step": 270
    },
    {
      "epoch": 1.396694214876033,
      "grad_norm": 0.0893397107720375,
      "learning_rate": 0.00010727580372250423,
      "loss": 3.7478,
      "step": 275
    },
    {
      "epoch": 1.4221233312142403,
      "grad_norm": 0.08886317163705826,
      "learning_rate": 0.00010558375634517767,
      "loss": 3.7491,
      "step": 280
    },
    {
      "epoch": 1.4475524475524475,
      "grad_norm": 0.07857801765203476,
      "learning_rate": 0.0001038917089678511,
      "loss": 3.7486,
      "step": 285
    },
    {
      "epoch": 1.472981563890655,
      "grad_norm": 0.07587987184524536,
      "learning_rate": 0.00010219966159052454,
      "loss": 3.7427,
      "step": 290
    },
    {
      "epoch": 1.498410680228862,
      "grad_norm": 0.07942982763051987,
      "learning_rate": 0.00010050761421319797,
      "loss": 3.7493,
      "step": 295
    },
    {
      "epoch": 1.5238397965670694,
      "grad_norm": 0.09064285457134247,
      "learning_rate": 9.881556683587141e-05,
      "loss": 3.7452,
      "step": 300
    },
    {
      "epoch": 1.5492689129052766,
      "grad_norm": 0.08269606530666351,
      "learning_rate": 9.712351945854485e-05,
      "loss": 3.7496,
      "step": 305
    },
    {
      "epoch": 1.5746980292434838,
      "grad_norm": 0.090977244079113,
      "learning_rate": 9.543147208121828e-05,
      "loss": 3.7417,
      "step": 310
    },
    {
      "epoch": 1.600127145581691,
      "grad_norm": 0.06558023393154144,
      "learning_rate": 9.373942470389172e-05,
      "loss": 3.7459,
      "step": 315
    },
    {
      "epoch": 1.6255562619198982,
      "grad_norm": 0.08327018469572067,
      "learning_rate": 9.204737732656514e-05,
      "loss": 3.7466,
      "step": 320
    },
    {
      "epoch": 1.6509853782581057,
      "grad_norm": 0.08627612888813019,
      "learning_rate": 9.035532994923858e-05,
      "loss": 3.7425,
      "step": 325
    },
    {
      "epoch": 1.6764144945963126,
      "grad_norm": 0.09870980679988861,
      "learning_rate": 8.866328257191202e-05,
      "loss": 3.7447,
      "step": 330
    },
    {
      "epoch": 1.70184361093452,
      "grad_norm": 0.08487703651189804,
      "learning_rate": 8.697123519458545e-05,
      "loss": 3.7436,
      "step": 335
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.07920721918344498,
      "learning_rate": 8.527918781725889e-05,
      "loss": 3.7456,
      "step": 340
    },
    {
      "epoch": 1.7527018436109345,
      "grad_norm": 0.10429544001817703,
      "learning_rate": 8.358714043993232e-05,
      "loss": 3.7474,
      "step": 345
    },
    {
      "epoch": 1.7781309599491417,
      "grad_norm": 0.10490645468235016,
      "learning_rate": 8.189509306260576e-05,
      "loss": 3.7484,
      "step": 350
    },
    {
      "epoch": 1.803560076287349,
      "grad_norm": 0.1072736531496048,
      "learning_rate": 8.020304568527919e-05,
      "loss": 3.7454,
      "step": 355
    },
    {
      "epoch": 1.8289891926255564,
      "grad_norm": 0.07580262422561646,
      "learning_rate": 7.851099830795263e-05,
      "loss": 3.7407,
      "step": 360
    },
    {
      "epoch": 1.8544183089637634,
      "grad_norm": 0.0910833403468132,
      "learning_rate": 7.681895093062607e-05,
      "loss": 3.7473,
      "step": 365
    },
    {
      "epoch": 1.8798474253019708,
      "grad_norm": 0.09519973397254944,
      "learning_rate": 7.51269035532995e-05,
      "loss": 3.7431,
      "step": 370
    },
    {
      "epoch": 1.905276541640178,
      "grad_norm": 0.11287260800600052,
      "learning_rate": 7.343485617597293e-05,
      "loss": 3.7433,
      "step": 375
    },
    {
      "epoch": 1.9307056579783852,
      "grad_norm": 0.08734976500272751,
      "learning_rate": 7.174280879864636e-05,
      "loss": 3.7483,
      "step": 380
    },
    {
      "epoch": 1.9561347743165927,
      "grad_norm": 0.11192352324724197,
      "learning_rate": 7.00507614213198e-05,
      "loss": 3.7437,
      "step": 385
    },
    {
      "epoch": 1.9815638906547997,
      "grad_norm": 0.09055870771408081,
      "learning_rate": 6.835871404399323e-05,
      "loss": 3.7459,
      "step": 390
    },
    {
      "epoch": 2.0050858232676414,
      "grad_norm": 0.0968381017446518,
      "learning_rate": 6.666666666666667e-05,
      "loss": 3.745,
      "step": 395
    },
    {
      "epoch": 2.030514939605849,
      "grad_norm": 0.1176806092262268,
      "learning_rate": 6.49746192893401e-05,
      "loss": 3.7453,
      "step": 400
    },
    {
      "epoch": 2.055944055944056,
      "grad_norm": 0.08491751551628113,
      "learning_rate": 6.328257191201354e-05,
      "loss": 3.7421,
      "step": 405
    },
    {
      "epoch": 2.0813731722822633,
      "grad_norm": 0.07876021414995193,
      "learning_rate": 6.159052453468698e-05,
      "loss": 3.7433,
      "step": 410
    },
    {
      "epoch": 2.1068022886204703,
      "grad_norm": 0.08320631086826324,
      "learning_rate": 5.989847715736041e-05,
      "loss": 3.746,
      "step": 415
    },
    {
      "epoch": 2.1322314049586777,
      "grad_norm": 0.09686998277902603,
      "learning_rate": 5.8206429780033846e-05,
      "loss": 3.7411,
      "step": 420
    },
    {
      "epoch": 2.157660521296885,
      "grad_norm": 0.09185737371444702,
      "learning_rate": 5.651438240270728e-05,
      "loss": 3.7398,
      "step": 425
    },
    {
      "epoch": 2.183089637635092,
      "grad_norm": 0.08767490833997726,
      "learning_rate": 5.482233502538071e-05,
      "loss": 3.7483,
      "step": 430
    },
    {
      "epoch": 2.2085187539732996,
      "grad_norm": 0.07524470239877701,
      "learning_rate": 5.313028764805415e-05,
      "loss": 3.7469,
      "step": 435
    },
    {
      "epoch": 2.2339478703115065,
      "grad_norm": 0.08454858511686325,
      "learning_rate": 5.143824027072758e-05,
      "loss": 3.7436,
      "step": 440
    },
    {
      "epoch": 2.259376986649714,
      "grad_norm": 0.08754666894674301,
      "learning_rate": 4.9746192893401014e-05,
      "loss": 3.7399,
      "step": 445
    },
    {
      "epoch": 2.284806102987921,
      "grad_norm": 0.08810237050056458,
      "learning_rate": 4.805414551607445e-05,
      "loss": 3.7374,
      "step": 450
    },
    {
      "epoch": 2.3102352193261284,
      "grad_norm": 0.0978853702545166,
      "learning_rate": 4.636209813874789e-05,
      "loss": 3.7409,
      "step": 455
    },
    {
      "epoch": 2.335664335664336,
      "grad_norm": 0.10187232494354248,
      "learning_rate": 4.467005076142132e-05,
      "loss": 3.7449,
      "step": 460
    },
    {
      "epoch": 2.361093452002543,
      "grad_norm": 0.08723073452711105,
      "learning_rate": 4.2978003384094756e-05,
      "loss": 3.7411,
      "step": 465
    },
    {
      "epoch": 2.3865225683407503,
      "grad_norm": 0.10127800703048706,
      "learning_rate": 4.128595600676819e-05,
      "loss": 3.7414,
      "step": 470
    },
    {
      "epoch": 2.4119516846789573,
      "grad_norm": 0.10045371949672699,
      "learning_rate": 3.959390862944163e-05,
      "loss": 3.7391,
      "step": 475
    },
    {
      "epoch": 2.4373808010171647,
      "grad_norm": 0.08165831863880157,
      "learning_rate": 3.7901861252115064e-05,
      "loss": 3.7434,
      "step": 480
    },
    {
      "epoch": 2.462809917355372,
      "grad_norm": 0.09652559459209442,
      "learning_rate": 3.62098138747885e-05,
      "loss": 3.7409,
      "step": 485
    },
    {
      "epoch": 2.488239033693579,
      "grad_norm": 0.10688949376344681,
      "learning_rate": 3.451776649746193e-05,
      "loss": 3.7382,
      "step": 490
    },
    {
      "epoch": 2.513668150031786,
      "grad_norm": 0.09586472809314728,
      "learning_rate": 3.2825719120135366e-05,
      "loss": 3.7429,
      "step": 495
    },
    {
      "epoch": 2.5390972663699936,
      "grad_norm": 0.09147757291793823,
      "learning_rate": 3.11336717428088e-05,
      "loss": 3.7395,
      "step": 500
    },
    {
      "epoch": 2.564526382708201,
      "grad_norm": 0.093789242208004,
      "learning_rate": 2.9441624365482233e-05,
      "loss": 3.7399,
      "step": 505
    },
    {
      "epoch": 2.589955499046408,
      "grad_norm": 0.09680703282356262,
      "learning_rate": 2.7749576988155667e-05,
      "loss": 3.7417,
      "step": 510
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 0.09710071235895157,
      "learning_rate": 2.6057529610829108e-05,
      "loss": 3.7402,
      "step": 515
    },
    {
      "epoch": 2.6408137317228224,
      "grad_norm": 0.10140563547611237,
      "learning_rate": 2.436548223350254e-05,
      "loss": 3.7429,
      "step": 520
    },
    {
      "epoch": 2.66624284806103,
      "grad_norm": 0.10678085684776306,
      "learning_rate": 2.2673434856175975e-05,
      "loss": 3.7427,
      "step": 525
    },
    {
      "epoch": 2.6916719643992373,
      "grad_norm": 0.08266333490610123,
      "learning_rate": 2.098138747884941e-05,
      "loss": 3.7443,
      "step": 530
    },
    {
      "epoch": 2.7171010807374443,
      "grad_norm": 0.08545656502246857,
      "learning_rate": 1.9289340101522843e-05,
      "loss": 3.7399,
      "step": 535
    },
    {
      "epoch": 2.7425301970756517,
      "grad_norm": 0.08810671418905258,
      "learning_rate": 1.759729272419628e-05,
      "loss": 3.7458,
      "step": 540
    },
    {
      "epoch": 2.7679593134138587,
      "grad_norm": 0.10238823294639587,
      "learning_rate": 1.5905245346869714e-05,
      "loss": 3.7429,
      "step": 545
    },
    {
      "epoch": 2.793388429752066,
      "grad_norm": 0.11100629717111588,
      "learning_rate": 1.421319796954315e-05,
      "loss": 3.7428,
      "step": 550
    },
    {
      "epoch": 2.8188175460902736,
      "grad_norm": 0.10883168876171112,
      "learning_rate": 1.2521150592216583e-05,
      "loss": 3.7455,
      "step": 555
    },
    {
      "epoch": 2.8442466624284806,
      "grad_norm": 0.09288147836923599,
      "learning_rate": 1.0829103214890017e-05,
      "loss": 3.7437,
      "step": 560
    },
    {
      "epoch": 2.869675778766688,
      "grad_norm": 0.07819854468107224,
      "learning_rate": 9.137055837563452e-06,
      "loss": 3.7376,
      "step": 565
    },
    {
      "epoch": 2.895104895104895,
      "grad_norm": 0.11040693521499634,
      "learning_rate": 7.445008460236887e-06,
      "loss": 3.7377,
      "step": 570
    },
    {
      "epoch": 2.9205340114431024,
      "grad_norm": 0.08074485510587692,
      "learning_rate": 5.7529610829103214e-06,
      "loss": 3.7406,
      "step": 575
    },
    {
      "epoch": 2.94596312778131,
      "grad_norm": 0.09581800550222397,
      "learning_rate": 4.060913705583756e-06,
      "loss": 3.7444,
      "step": 580
    },
    {
      "epoch": 2.971392244119517,
      "grad_norm": 0.10903042554855347,
      "learning_rate": 2.3688663282571915e-06,
      "loss": 3.7422,
      "step": 585
    },
    {
      "epoch": 2.996821360457724,
      "grad_norm": 0.08204720914363861,
      "learning_rate": 6.76818950930626e-07,
      "loss": 3.7478,
      "step": 590
    }
  ],
  "logging_steps": 5,
  "max_steps": 591,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.178462679418368e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
